{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "specified-prairie",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "devoted-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-study",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "seven-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('dataset/Shoes - All - Train.npz')\n",
    "validation_data = np.load('dataset/Shoes - All - Validation.npz')\n",
    "test_data = np.load('dataset/Shoes - All - Test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "psychological-compiler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images', 'labels', 'genders']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "posted-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train, genders_train = train_data['images'], train_data['genders']\n",
    "images_val, genders_val = validation_data['images'], validation_data['genders']\n",
    "images_test, genders_test = test_data['images'], test_data['genders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "practical-surgery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4959, 120, 90, 3)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "independent-million",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4959,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genders_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "stock-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "# Flip the image by flip the axis 2\n",
    "# we have 4 axis now in images (no. of pic, y, x, channel)\n",
    "# so, if we want to flip horizontally, we flip axis 2\n",
    "\n",
    "flipped_images_train = np.flip(images_train, axis = 2)\n",
    "flipped_images_val = np.flip(images_val, axis = 2)\n",
    "flipped_images_test = np.flip(images_test, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "stone-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to concatenate labels with itself, cuz we increase pic in images, so labels have to be increased too.\n",
    "images_train = np.concatenate((images_train, flipped_images_train))\n",
    "genders_train = np.concatenate((genders_train, genders_train))\n",
    "\n",
    "images_val = np.concatenate((images_val, flipped_images_val))\n",
    "genders_val = np.concatenate((genders_val, genders_val))\n",
    "\n",
    "images_test = np.concatenate((images_test, flipped_images_test))\n",
    "genders_test = np.concatenate((genders_test, genders_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "sensitive-absorption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1238, 120, 90, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "center-recording",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1238,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genders_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "brief-insert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1238, 120, 90, 3)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "continental-organ",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1238,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genders_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "lonely-shadow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(genders_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "immediate-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "images_train = images_train/255.0\n",
    "images_val = images_val/255.0\n",
    "images_test = images_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-indie",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-medium",
   "metadata": {},
   "source": [
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "computational-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "former-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_KERNEL_SIZE = hp.HParam('kernel size', hp.Discrete([3,5]))\n",
    "HP_KERNEL_NUM = hp.HParam('kernel num', hp.Discrete([32,64,128]))\n",
    "HP_LAMBDA_REG = hp.HParam('lambda reg', hp.Discrete([1e-5,1e-4,1e-3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "devoted-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer(r'logs/Model 1 (Shoes_Gender_L2)/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams = [HP_KERNEL_SIZE, HP_KERNEL_NUM, HP_LAMBDA_REG],\n",
    "        metrics = [hp.Metric(METRIC, display_name = 'accuracy')]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-scratch",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "friendly-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams, session_num):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(hparams[HP_KERNEL_NUM], hparams[HP_KERNEL_SIZE], activation = 'relu', input_shape = (120,90,3), kernel_regularizer = tf.keras.regularizers.L2(hparams[HP_LAMBDA_REG])),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Conv2D(hparams[HP_KERNEL_NUM], hparams[HP_KERNEL_SIZE], activation = 'relu', kernel_regularizer = tf.keras.regularizers.L2(hparams[HP_LAMBDA_REG])),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation = 'relu', kernel_regularizer = tf.keras.regularizers.L2(hparams[HP_LAMBDA_REG])),\n",
    "        tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "    ])\n",
    "    \n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(name = 'sparse_categorical_crossentropy')\n",
    "    model.compile(optimizer = 'adam', loss = loss_fn, metrics = ['accuracy','sparse_categorical_crossentropy'])\n",
    "    \n",
    "    log_dir = \"Logs\\\\Model 1 (Shoes_Gender_L2)\\\\fit\\\\\" + f\"run-{session_num}\"\n",
    "    \n",
    "    # functions for creating confusion matrix\n",
    "    def plot_confusion_matrix(cm, class_names):\n",
    "        \"\"\"\n",
    "        Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "        Args:\n",
    "        cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "        class_names (array, shape = [n]): String names of the integer classes\n",
    "        \"\"\"\n",
    "        figure = plt.figure(figsize=(12, 12))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title(\"Confusion matrix\")\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names, rotation=45)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "\n",
    "        # Normalize the confusion matrix.\n",
    "        cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "        # Use white text if squares are dark; otherwise black.\n",
    "        threshold = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "        return figure\n",
    "\n",
    "    def plot_to_image(figure):\n",
    "        \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "        returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "\n",
    "        # Save the plot to a PNG in memory.\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "\n",
    "        # Closing the figure prevents it from being displayed directly inside the notebook.\n",
    "        plt.close(figure)\n",
    "\n",
    "        buf.seek(0)\n",
    "\n",
    "        # Convert PNG buffer to TF image\n",
    "        image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "\n",
    "        # Add the batch dimension\n",
    "        image = tf.expand_dims(image, 0)\n",
    "\n",
    "        return image\n",
    "\n",
    "    # Define a file writer variable for logging purposes\n",
    "    file_writer_cm = tf.summary.create_file_writer(log_dir + '/cm')\n",
    "\n",
    "    def log_confusion_matrix(epoch, logs):\n",
    "        # Use the model to predict the values from the validation dataset.\n",
    "        test_pred_raw = model.predict(images_val)\n",
    "        test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "        # Calculate the confusion matrix.\n",
    "        cm = sklearn.metrics.confusion_matrix(genders_val, test_pred)\n",
    "\n",
    "        # Log the confusion matrix as an image summary.\n",
    "        figure = plot_confusion_matrix(cm, class_names=['Male','Female'])\n",
    "        cm_image = plot_to_image(figure)\n",
    "\n",
    "        # Log the confusion matrix as an image summary.\n",
    "        with file_writer_cm.as_default():\n",
    "            tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\n",
    "            \n",
    "    # callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_sparse_categorical_crossentropy', patience = 2, restore_best_weights = True)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1, profile_batch = 0)\n",
    "    cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end = log_confusion_matrix)\n",
    "    \n",
    "    model.fit(images_train,\n",
    "              genders_train,\n",
    "              epochs = EPOCHS,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              callbacks = [tensorboard_callback, cm_callback, early_stopping],\n",
    "              validation_data = (images_val, genders_val),\n",
    "              verbose = 1)\n",
    "    \n",
    "    _, accuracy, _ = model.evaluate(images_val, genders_val)\n",
    "    \n",
    "    model.save(r'saved_models\\Model 1 (Shoes_Gender_L2)\\Run-{}'.format(session_num))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "authentic-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(log_dir, hparams, session_num):\n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = train_test_model(hparams, session_num)\n",
    "        tf.summary.scalar(METRIC,accuracy,step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "diverse-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Trial: run-1\n",
      "{'kernel size': 3, 'kernel num': 32, 'lambda reg': 1e-05}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 51s 329ms/step - loss: 0.5942 - accuracy: 0.7309 - sparse_categorical_crossentropy: 0.5892 - val_loss: 0.5210 - val_accuracy: 0.7488 - val_sparse_categorical_crossentropy: 0.5159\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 51s 326ms/step - loss: 0.4131 - accuracy: 0.8107 - sparse_categorical_crossentropy: 0.4077 - val_loss: 0.4090 - val_accuracy: 0.8118 - val_sparse_categorical_crossentropy: 0.4032\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 54s 349ms/step - loss: 0.3662 - accuracy: 0.8366 - sparse_categorical_crossentropy: 0.3599 - val_loss: 0.4054 - val_accuracy: 0.8166 - val_sparse_categorical_crossentropy: 0.3987\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 55s 352ms/step - loss: 0.3311 - accuracy: 0.8532 - sparse_categorical_crossentropy: 0.3240 - val_loss: 0.4173 - val_accuracy: 0.8223 - val_sparse_categorical_crossentropy: 0.4098\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 51s 328ms/step - loss: 0.3218 - accuracy: 0.8608 - sparse_categorical_crossentropy: 0.3136 - val_loss: 0.4159 - val_accuracy: 0.8118 - val_sparse_categorical_crossentropy: 0.4072\n",
      "39/39 [==============================] - 2s 48ms/step - loss: 0.4054 - accuracy: 0.8166 - sparse_categorical_crossentropy: 0.3987\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\envs\\Python3-TF2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\envs\\Python3-TF2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-1\\assets\n",
      "--- Starting Trial: run-2\n",
      "{'kernel size': 3, 'kernel num': 32, 'lambda reg': 0.0001}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 54s 347ms/step - loss: 0.5504 - accuracy: 0.7561 - sparse_categorical_crossentropy: 0.5251 - val_loss: 0.4405 - val_accuracy: 0.8086 - val_sparse_categorical_crossentropy: 0.4182\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 54s 350ms/step - loss: 0.4297 - accuracy: 0.8107 - sparse_categorical_crossentropy: 0.4058 - val_loss: 0.5009 - val_accuracy: 0.7876 - val_sparse_categorical_crossentropy: 0.4764\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 54s 347ms/step - loss: 0.3966 - accuracy: 0.8307 - sparse_categorical_crossentropy: 0.3703 - val_loss: 0.4299 - val_accuracy: 0.8199 - val_sparse_categorical_crossentropy: 0.4022\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 54s 348ms/step - loss: 0.3610 - accuracy: 0.8500 - sparse_categorical_crossentropy: 0.3324 - val_loss: 0.4211 - val_accuracy: 0.8215 - val_sparse_categorical_crossentropy: 0.3921\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 51s 330ms/step - loss: 0.3338 - accuracy: 0.8632 - sparse_categorical_crossentropy: 0.3026 - val_loss: 0.4448 - val_accuracy: 0.8255 - val_sparse_categorical_crossentropy: 0.4123\n",
      "Epoch 6/15\n",
      "155/155 [==============================] - 61s 393ms/step - loss: 0.3214 - accuracy: 0.8736 - sparse_categorical_crossentropy: 0.2871 - val_loss: 0.4428 - val_accuracy: 0.8376 - val_sparse_categorical_crossentropy: 0.4062\n",
      "39/39 [==============================] - 2s 57ms/step - loss: 0.4211 - accuracy: 0.8215 - sparse_categorical_crossentropy: 0.3921\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-2\\assets\n",
      "--- Starting Trial: run-3\n",
      "{'kernel size': 3, 'kernel num': 32, 'lambda reg': 0.001}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 56s 363ms/step - loss: 0.6917 - accuracy: 0.7407 - sparse_categorical_crossentropy: 0.5496 - val_loss: 0.5469 - val_accuracy: 0.7948 - val_sparse_categorical_crossentropy: 0.4556\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 60s 385ms/step - loss: 0.5039 - accuracy: 0.8010 - sparse_categorical_crossentropy: 0.4264 - val_loss: 0.4840 - val_accuracy: 0.8005 - val_sparse_categorical_crossentropy: 0.4175\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 56s 362ms/step - loss: 0.4594 - accuracy: 0.8185 - sparse_categorical_crossentropy: 0.3957 - val_loss: 0.4740 - val_accuracy: 0.8037 - val_sparse_categorical_crossentropy: 0.4127\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 56s 361ms/step - loss: 0.4416 - accuracy: 0.8286 - sparse_categorical_crossentropy: 0.3778 - val_loss: 0.4832 - val_accuracy: 0.8013 - val_sparse_categorical_crossentropy: 0.4170\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 55s 355ms/step - loss: 0.4309 - accuracy: 0.8324 - sparse_categorical_crossentropy: 0.3660 - val_loss: 0.4864 - val_accuracy: 0.8045 - val_sparse_categorical_crossentropy: 0.4223\n",
      "39/39 [==============================] - 2s 54ms/step - loss: 0.4740 - accuracy: 0.8037 - sparse_categorical_crossentropy: 0.4127\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-3\\assets\n",
      "--- Starting Trial: run-4\n",
      "{'kernel size': 3, 'kernel num': 64, 'lambda reg': 1e-05}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 109s 705ms/step - loss: 0.5931 - accuracy: 0.7444 - sparse_categorical_crossentropy: 0.5883 - val_loss: 0.4865 - val_accuracy: 0.7876 - val_sparse_categorical_crossentropy: 0.4819\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 105s 680ms/step - loss: 0.4196 - accuracy: 0.8080 - sparse_categorical_crossentropy: 0.4146 - val_loss: 0.4388 - val_accuracy: 0.8029 - val_sparse_categorical_crossentropy: 0.4335\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 105s 678ms/step - loss: 0.3564 - accuracy: 0.8399 - sparse_categorical_crossentropy: 0.3507 - val_loss: 0.4131 - val_accuracy: 0.8142 - val_sparse_categorical_crossentropy: 0.4070\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 103s 666ms/step - loss: 0.3316 - accuracy: 0.8547 - sparse_categorical_crossentropy: 0.3250 - val_loss: 0.4124 - val_accuracy: 0.8312 - val_sparse_categorical_crossentropy: 0.4053\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 101s 652ms/step - loss: 0.3011 - accuracy: 0.8672 - sparse_categorical_crossentropy: 0.2933 - val_loss: 0.4224 - val_accuracy: 0.8255 - val_sparse_categorical_crossentropy: 0.4139\n",
      "Epoch 6/15\n",
      "155/155 [==============================] - 102s 656ms/step - loss: 0.2671 - accuracy: 0.8856 - sparse_categorical_crossentropy: 0.2580 - val_loss: 0.4581 - val_accuracy: 0.8417 - val_sparse_categorical_crossentropy: 0.4483\n",
      "39/39 [==============================] - 4s 90ms/step - loss: 0.4124 - accuracy: 0.8312 - sparse_categorical_crossentropy: 0.4053\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-4\\assets\n",
      "--- Starting Trial: run-5\n",
      "{'kernel size': 3, 'kernel num': 64, 'lambda reg': 0.0001}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 102s 660ms/step - loss: 0.6261 - accuracy: 0.7355 - sparse_categorical_crossentropy: 0.5942 - val_loss: 0.4806 - val_accuracy: 0.7876 - val_sparse_categorical_crossentropy: 0.4551\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 102s 659ms/step - loss: 0.4402 - accuracy: 0.8141 - sparse_categorical_crossentropy: 0.4155 - val_loss: 0.4485 - val_accuracy: 0.8142 - val_sparse_categorical_crossentropy: 0.4242\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 103s 662ms/step - loss: 0.4258 - accuracy: 0.8152 - sparse_categorical_crossentropy: 0.3992 - val_loss: 0.4320 - val_accuracy: 0.8191 - val_sparse_categorical_crossentropy: 0.4036\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 102s 657ms/step - loss: 0.3815 - accuracy: 0.8413 - sparse_categorical_crossentropy: 0.3534 - val_loss: 0.4447 - val_accuracy: 0.8174 - val_sparse_categorical_crossentropy: 0.4165\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 102s 655ms/step - loss: 0.3484 - accuracy: 0.8537 - sparse_categorical_crossentropy: 0.3201 - val_loss: 0.4245 - val_accuracy: 0.8255 - val_sparse_categorical_crossentropy: 0.3959\n",
      "Epoch 6/15\n",
      "155/155 [==============================] - 102s 659ms/step - loss: 0.3197 - accuracy: 0.8663 - sparse_categorical_crossentropy: 0.2900 - val_loss: 0.4530 - val_accuracy: 0.8247 - val_sparse_categorical_crossentropy: 0.4220\n",
      "Epoch 7/15\n",
      "155/155 [==============================] - 104s 668ms/step - loss: 0.2950 - accuracy: 0.8844 - sparse_categorical_crossentropy: 0.2626 - val_loss: 0.4382 - val_accuracy: 0.8465 - val_sparse_categorical_crossentropy: 0.4040\n",
      "39/39 [==============================] - 3s 88ms/step - loss: 0.4245 - accuracy: 0.8255 - sparse_categorical_crossentropy: 0.3959\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-5\\assets\n",
      "--- Starting Trial: run-6\n",
      "{'kernel size': 3, 'kernel num': 64, 'lambda reg': 0.001}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 106s 683ms/step - loss: 0.8343 - accuracy: 0.7239 - sparse_categorical_crossentropy: 0.6207 - val_loss: 0.6091 - val_accuracy: 0.7787 - val_sparse_categorical_crossentropy: 0.4748\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 107s 690ms/step - loss: 0.5573 - accuracy: 0.7906 - sparse_categorical_crossentropy: 0.4525 - val_loss: 0.5165 - val_accuracy: 0.7956 - val_sparse_categorical_crossentropy: 0.4317\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 106s 681ms/step - loss: 0.4840 - accuracy: 0.8117 - sparse_categorical_crossentropy: 0.4055 - val_loss: 0.4882 - val_accuracy: 0.8053 - val_sparse_categorical_crossentropy: 0.4150\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 106s 681ms/step - loss: 0.4611 - accuracy: 0.8217 - sparse_categorical_crossentropy: 0.3899 - val_loss: 0.4785 - val_accuracy: 0.8191 - val_sparse_categorical_crossentropy: 0.4090\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 107s 689ms/step - loss: 0.4412 - accuracy: 0.8327 - sparse_categorical_crossentropy: 0.3731 - val_loss: 0.4688 - val_accuracy: 0.8199 - val_sparse_categorical_crossentropy: 0.4020\n",
      "Epoch 6/15\n",
      "155/155 [==============================] - 107s 688ms/step - loss: 0.4243 - accuracy: 0.8374 - sparse_categorical_crossentropy: 0.3572 - val_loss: 0.4797 - val_accuracy: 0.8118 - val_sparse_categorical_crossentropy: 0.4118\n",
      "Epoch 7/15\n",
      "155/155 [==============================] - 115s 741ms/step - loss: 0.4068 - accuracy: 0.8466 - sparse_categorical_crossentropy: 0.3383 - val_loss: 0.4507 - val_accuracy: 0.8199 - val_sparse_categorical_crossentropy: 0.3828\n",
      "Epoch 8/15\n",
      "155/155 [==============================] - 106s 687ms/step - loss: 0.3994 - accuracy: 0.8516 - sparse_categorical_crossentropy: 0.3303 - val_loss: 0.4719 - val_accuracy: 0.8166 - val_sparse_categorical_crossentropy: 0.4015\n",
      "Epoch 9/15\n",
      "155/155 [==============================] - 101s 649ms/step - loss: 0.4023 - accuracy: 0.8507 - sparse_categorical_crossentropy: 0.3296 - val_loss: 0.4705 - val_accuracy: 0.8320 - val_sparse_categorical_crossentropy: 0.3963\n",
      "39/39 [==============================] - 3s 84ms/step - loss: 0.4507 - accuracy: 0.8199 - sparse_categorical_crossentropy: 0.3828 1s - loss: 0.4777 - accuracy\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-6\\assets\n",
      "--- Starting Trial: run-7\n",
      "{'kernel size': 3, 'kernel num': 128, 'lambda reg': 1e-05}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 231s 1s/step - loss: 0.6047 - accuracy: 0.7431 - sparse_categorical_crossentropy: 0.5988 - val_loss: 0.4485 - val_accuracy: 0.7997 - val_sparse_categorical_crossentropy: 0.4432\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 263s 2s/step - loss: 0.4325 - accuracy: 0.8043 - sparse_categorical_crossentropy: 0.4273 - val_loss: 0.4965 - val_accuracy: 0.7859 - val_sparse_categorical_crossentropy: 0.4914\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 270s 2s/step - loss: 0.3898 - accuracy: 0.8245 - sparse_categorical_crossentropy: 0.3845 - val_loss: 0.4237 - val_accuracy: 0.8094 - val_sparse_categorical_crossentropy: 0.4183\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 275s 2s/step - loss: 0.3574 - accuracy: 0.8394 - sparse_categorical_crossentropy: 0.3517 - val_loss: 0.4248 - val_accuracy: 0.8183 - val_sparse_categorical_crossentropy: 0.4182\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 285s 2s/step - loss: 0.3407 - accuracy: 0.8531 - sparse_categorical_crossentropy: 0.3335 - val_loss: 0.4149 - val_accuracy: 0.8255 - val_sparse_categorical_crossentropy: 0.4072\n",
      "Epoch 6/15\n",
      "155/155 [==============================] - 288s 2s/step - loss: 0.2970 - accuracy: 0.8713 - sparse_categorical_crossentropy: 0.2887 - val_loss: 0.4153 - val_accuracy: 0.8352 - val_sparse_categorical_crossentropy: 0.4064\n",
      "Epoch 7/15\n",
      "155/155 [==============================] - 263s 2s/step - loss: 0.2774 - accuracy: 0.8806 - sparse_categorical_crossentropy: 0.2677 - val_loss: 0.4176 - val_accuracy: 0.8457 - val_sparse_categorical_crossentropy: 0.4070\n",
      "Epoch 8/15\n",
      "155/155 [==============================] - 255s 2s/step - loss: 0.2361 - accuracy: 0.8977 - sparse_categorical_crossentropy: 0.2249 - val_loss: 0.4368 - val_accuracy: 0.8578 - val_sparse_categorical_crossentropy: 0.4251\n",
      "39/39 [==============================] - 9s 230ms/step - loss: 0.4153 - accuracy: 0.8352 - sparse_categorical_crossentropy: 0.4064\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-7\\assets\n",
      "--- Starting Trial: run-8\n",
      "{'kernel size': 3, 'kernel num': 128, 'lambda reg': 0.0001}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 258s 2s/step - loss: 0.6891 - accuracy: 0.7218 - sparse_categorical_crossentropy: 0.6485 - val_loss: 0.4896 - val_accuracy: 0.7754 - val_sparse_categorical_crossentropy: 0.4561\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 256s 2s/step - loss: 0.4580 - accuracy: 0.8046 - sparse_categorical_crossentropy: 0.4272 - val_loss: 0.4378 - val_accuracy: 0.8053 - val_sparse_categorical_crossentropy: 0.4092\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 256s 2s/step - loss: 0.4108 - accuracy: 0.8236 - sparse_categorical_crossentropy: 0.3822 - val_loss: 0.4257 - val_accuracy: 0.8102 - val_sparse_categorical_crossentropy: 0.3972\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 254s 2s/step - loss: 0.3691 - accuracy: 0.8431 - sparse_categorical_crossentropy: 0.3400 - val_loss: 0.4238 - val_accuracy: 0.8207 - val_sparse_categorical_crossentropy: 0.3933\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 256s 2s/step - loss: 0.3512 - accuracy: 0.8541 - sparse_categorical_crossentropy: 0.3194 - val_loss: 0.4484 - val_accuracy: 0.8255 - val_sparse_categorical_crossentropy: 0.4153\n",
      "Epoch 6/15\n",
      "155/155 [==============================] - 255s 2s/step - loss: 0.3201 - accuracy: 0.8707 - sparse_categorical_crossentropy: 0.2853 - val_loss: 0.4390 - val_accuracy: 0.8263 - val_sparse_categorical_crossentropy: 0.4029\n",
      "39/39 [==============================] - 9s 224ms/step - loss: 0.4238 - accuracy: 0.8207 - sparse_categorical_crossentropy: 0.3933\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-8\\assets\n",
      "--- Starting Trial: run-9\n",
      "{'kernel size': 3, 'kernel num': 128, 'lambda reg': 0.001}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 257s 2s/step - loss: 0.8390 - accuracy: 0.7321 - sparse_categorical_crossentropy: 0.6067 - val_loss: 0.5643 - val_accuracy: 0.7997 - val_sparse_categorical_crossentropy: 0.4399\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 256s 2s/step - loss: 0.5669 - accuracy: 0.7820 - sparse_categorical_crossentropy: 0.4713 - val_loss: 0.5579 - val_accuracy: 0.7916 - val_sparse_categorical_crossentropy: 0.4789\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 257s 2s/step - loss: 0.4791 - accuracy: 0.8138 - sparse_categorical_crossentropy: 0.4094 - val_loss: 0.4959 - val_accuracy: 0.8045 - val_sparse_categorical_crossentropy: 0.4322\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 256s 2s/step - loss: 0.4507 - accuracy: 0.8263 - sparse_categorical_crossentropy: 0.3887 - val_loss: 0.4744 - val_accuracy: 0.8126 - val_sparse_categorical_crossentropy: 0.4144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15\n",
      "155/155 [==============================] - 256s 2s/step - loss: 0.4275 - accuracy: 0.8329 - sparse_categorical_crossentropy: 0.3672 - val_loss: 0.4683 - val_accuracy: 0.8142 - val_sparse_categorical_crossentropy: 0.4089\n",
      "Epoch 6/15\n",
      "155/155 [==============================] - 256s 2s/step - loss: 0.4172 - accuracy: 0.8366 - sparse_categorical_crossentropy: 0.3561 - val_loss: 0.4702 - val_accuracy: 0.8183 - val_sparse_categorical_crossentropy: 0.4075\n",
      "Epoch 7/15\n",
      "155/155 [==============================] - 256s 2s/step - loss: 0.4063 - accuracy: 0.8474 - sparse_categorical_crossentropy: 0.3431 - val_loss: 0.4555 - val_accuracy: 0.8247 - val_sparse_categorical_crossentropy: 0.3914\n",
      "Epoch 8/15\n",
      "155/155 [==============================] - 255s 2s/step - loss: 0.3849 - accuracy: 0.8537 - sparse_categorical_crossentropy: 0.3194 - val_loss: 0.4530 - val_accuracy: 0.8304 - val_sparse_categorical_crossentropy: 0.3876\n",
      "Epoch 9/15\n",
      "155/155 [==============================] - 252s 2s/step - loss: 0.3780 - accuracy: 0.8590 - sparse_categorical_crossentropy: 0.3112 - val_loss: 0.4579 - val_accuracy: 0.8312 - val_sparse_categorical_crossentropy: 0.3900\n",
      "Epoch 10/15\n",
      "155/155 [==============================] - 252s 2s/step - loss: 0.3786 - accuracy: 0.8634 - sparse_categorical_crossentropy: 0.3079 - val_loss: 0.4710 - val_accuracy: 0.8368 - val_sparse_categorical_crossentropy: 0.4004\n",
      "39/39 [==============================] - 9s 229ms/step - loss: 0.4530 - accuracy: 0.8304 - sparse_categorical_crossentropy: 0.3876\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-9\\assets\n",
      "--- Starting Trial: run-10\n",
      "{'kernel size': 5, 'kernel num': 32, 'lambda reg': 1e-05}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 83s 534ms/step - loss: 0.5979 - accuracy: 0.7225 - sparse_categorical_crossentropy: 0.5943 - val_loss: 0.4723 - val_accuracy: 0.7843 - val_sparse_categorical_crossentropy: 0.4693\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 82s 531ms/step - loss: 0.4528 - accuracy: 0.7959 - sparse_categorical_crossentropy: 0.4499 - val_loss: 0.4431 - val_accuracy: 0.7916 - val_sparse_categorical_crossentropy: 0.4401\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 82s 530ms/step - loss: 0.4379 - accuracy: 0.7994 - sparse_categorical_crossentropy: 0.4347 - val_loss: 0.4387 - val_accuracy: 0.8053 - val_sparse_categorical_crossentropy: 0.4354\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 84s 541ms/step - loss: 0.3942 - accuracy: 0.8198 - sparse_categorical_crossentropy: 0.3907 - val_loss: 0.4141 - val_accuracy: 0.8142 - val_sparse_categorical_crossentropy: 0.4103\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 83s 536ms/step - loss: 0.3563 - accuracy: 0.8401 - sparse_categorical_crossentropy: 0.3523 - val_loss: 0.4184 - val_accuracy: 0.8069 - val_sparse_categorical_crossentropy: 0.4142\n",
      "Epoch 6/15\n",
      "155/155 [==============================] - 83s 536ms/step - loss: 0.3349 - accuracy: 0.8510 - sparse_categorical_crossentropy: 0.3304 - val_loss: 0.4182 - val_accuracy: 0.8215 - val_sparse_categorical_crossentropy: 0.4134\n",
      "39/39 [==============================] - 3s 70ms/step - loss: 0.4141 - accuracy: 0.8142 - sparse_categorical_crossentropy: 0.4103\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-10\\assets\n",
      "--- Starting Trial: run-11\n",
      "{'kernel size': 5, 'kernel num': 32, 'lambda reg': 0.0001}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 85s 546ms/step - loss: 0.6018 - accuracy: 0.7289 - sparse_categorical_crossentropy: 0.5705 - val_loss: 0.5185 - val_accuracy: 0.7714 - val_sparse_categorical_crossentropy: 0.4938\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 83s 537ms/step - loss: 0.4645 - accuracy: 0.8016 - sparse_categorical_crossentropy: 0.4412 - val_loss: 0.4407 - val_accuracy: 0.8102 - val_sparse_categorical_crossentropy: 0.4182\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 84s 539ms/step - loss: 0.4106 - accuracy: 0.8264 - sparse_categorical_crossentropy: 0.3885 - val_loss: 0.4502 - val_accuracy: 0.7989 - val_sparse_categorical_crossentropy: 0.4282\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 84s 540ms/step - loss: 0.3858 - accuracy: 0.8358 - sparse_categorical_crossentropy: 0.3637 - val_loss: 0.4380 - val_accuracy: 0.8215 - val_sparse_categorical_crossentropy: 0.4156\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 83s 538ms/step - loss: 0.3759 - accuracy: 0.8405 - sparse_categorical_crossentropy: 0.3522 - val_loss: 0.4466 - val_accuracy: 0.8134 - val_sparse_categorical_crossentropy: 0.4203\n",
      "Epoch 6/15\n",
      "155/155 [==============================] - 83s 538ms/step - loss: 0.3526 - accuracy: 0.8537 - sparse_categorical_crossentropy: 0.3262 - val_loss: 0.4496 - val_accuracy: 0.8166 - val_sparse_categorical_crossentropy: 0.4229\n",
      "39/39 [==============================] - 3s 70ms/step - loss: 0.4380 - accuracy: 0.8215 - sparse_categorical_crossentropy: 0.4156\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-11\\assets\n",
      "--- Starting Trial: run-12\n",
      "{'kernel size': 5, 'kernel num': 32, 'lambda reg': 0.001}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 83s 538ms/step - loss: 0.7098 - accuracy: 0.7314 - sparse_categorical_crossentropy: 0.5692 - val_loss: 0.5533 - val_accuracy: 0.7819 - val_sparse_categorical_crossentropy: 0.4799\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 84s 544ms/step - loss: 0.5315 - accuracy: 0.7867 - sparse_categorical_crossentropy: 0.4725 - val_loss: 0.5173 - val_accuracy: 0.7892 - val_sparse_categorical_crossentropy: 0.4679\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 82s 531ms/step - loss: 0.5052 - accuracy: 0.8007 - sparse_categorical_crossentropy: 0.4589 - val_loss: 0.5755 - val_accuracy: 0.7682 - val_sparse_categorical_crossentropy: 0.5228\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 82s 529ms/step - loss: 0.4682 - accuracy: 0.8116 - sparse_categorical_crossentropy: 0.4228 - val_loss: 0.5277 - val_accuracy: 0.7722 - val_sparse_categorical_crossentropy: 0.4861\n",
      "39/39 [==============================] - 3s 68ms/step - loss: 0.5173 - accuracy: 0.7892 - sparse_categorical_crossentropy: 0.4679\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-12\\assets\n",
      "--- Starting Trial: run-13\n",
      "{'kernel size': 5, 'kernel num': 64, 'lambda reg': 1e-05}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 170s 1s/step - loss: 0.6268 - accuracy: 0.7344 - sparse_categorical_crossentropy: 0.6220 - val_loss: 0.4767 - val_accuracy: 0.7859 - val_sparse_categorical_crossentropy: 0.4724\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 169s 1s/step - loss: 0.4918 - accuracy: 0.7801 - sparse_categorical_crossentropy: 0.4874 - val_loss: 0.4542 - val_accuracy: 0.8005 - val_sparse_categorical_crossentropy: 0.4494\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 169s 1s/step - loss: 0.4184 - accuracy: 0.8172 - sparse_categorical_crossentropy: 0.4133 - val_loss: 0.4380 - val_accuracy: 0.8118 - val_sparse_categorical_crossentropy: 0.4325\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 172s 1s/step - loss: 0.3586 - accuracy: 0.8385 - sparse_categorical_crossentropy: 0.3529 - val_loss: 0.4281 - val_accuracy: 0.8174 - val_sparse_categorical_crossentropy: 0.4220\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 169s 1s/step - loss: 0.3187 - accuracy: 0.8581 - sparse_categorical_crossentropy: 0.3123 - val_loss: 0.4212 - val_accuracy: 0.8207 - val_sparse_categorical_crossentropy: 0.4144\n",
      "Epoch 6/15\n",
      "155/155 [==============================] - 170s 1s/step - loss: 0.2873 - accuracy: 0.8756 - sparse_categorical_crossentropy: 0.2800 - val_loss: 0.4422 - val_accuracy: 0.8328 - val_sparse_categorical_crossentropy: 0.4345\n",
      "Epoch 7/15\n",
      "155/155 [==============================] - 169s 1s/step - loss: 0.2516 - accuracy: 0.8913 - sparse_categorical_crossentropy: 0.2433 - val_loss: 0.4362 - val_accuracy: 0.8279 - val_sparse_categorical_crossentropy: 0.4273\n",
      "39/39 [==============================] - 6s 145ms/step - loss: 0.4212 - accuracy: 0.8207 - sparse_categorical_crossentropy: 0.4144\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-13\\assets\n",
      "--- Starting Trial: run-14\n",
      "{'kernel size': 5, 'kernel num': 64, 'lambda reg': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "155/155 [==============================] - 170s 1s/step - loss: 0.7242 - accuracy: 0.7222 - sparse_categorical_crossentropy: 0.6860 - val_loss: 0.5080 - val_accuracy: 0.7795 - val_sparse_categorical_crossentropy: 0.4763\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 170s 1s/step - loss: 0.4914 - accuracy: 0.7902 - sparse_categorical_crossentropy: 0.4624 - val_loss: 0.4802 - val_accuracy: 0.7876 - val_sparse_categorical_crossentropy: 0.4534\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 171s 1s/step - loss: 0.4448 - accuracy: 0.8083 - sparse_categorical_crossentropy: 0.4197 - val_loss: 0.4540 - val_accuracy: 0.8029 - val_sparse_categorical_crossentropy: 0.4303\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 170s 1s/step - loss: 0.4090 - accuracy: 0.8243 - sparse_categorical_crossentropy: 0.3859 - val_loss: 0.4444 - val_accuracy: 0.7956 - val_sparse_categorical_crossentropy: 0.4220\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 171s 1s/step - loss: 0.3753 - accuracy: 0.8396 - sparse_categorical_crossentropy: 0.3531 - val_loss: 0.4223 - val_accuracy: 0.8158 - val_sparse_categorical_crossentropy: 0.4006\n",
      "Epoch 6/15\n",
      "155/155 [==============================] - 170s 1s/step - loss: 0.3584 - accuracy: 0.8488 - sparse_categorical_crossentropy: 0.3366 - val_loss: 0.4208 - val_accuracy: 0.8255 - val_sparse_categorical_crossentropy: 0.3988\n",
      "Epoch 7/15\n",
      "155/155 [==============================] - 170s 1s/step - loss: 0.3363 - accuracy: 0.8593 - sparse_categorical_crossentropy: 0.3140 - val_loss: 0.4058 - val_accuracy: 0.8352 - val_sparse_categorical_crossentropy: 0.3830\n",
      "Epoch 8/15\n",
      "155/155 [==============================] - 170s 1s/step - loss: 0.3121 - accuracy: 0.8709 - sparse_categorical_crossentropy: 0.2887 - val_loss: 0.4151 - val_accuracy: 0.8368 - val_sparse_categorical_crossentropy: 0.3912\n",
      "Epoch 9/15\n",
      "155/155 [==============================] - 170s 1s/step - loss: 0.2937 - accuracy: 0.8802 - sparse_categorical_crossentropy: 0.2686 - val_loss: 0.4145 - val_accuracy: 0.8433 - val_sparse_categorical_crossentropy: 0.3881\n",
      "39/39 [==============================] - 6s 145ms/step - loss: 0.4058 - accuracy: 0.8352 - sparse_categorical_crossentropy: 0.3830\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-14\\assets\n",
      "--- Starting Trial: run-15\n",
      "{'kernel size': 5, 'kernel num': 64, 'lambda reg': 0.001}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 197s 1s/step - loss: 0.8004 - accuracy: 0.7142 - sparse_categorical_crossentropy: 0.6053 - val_loss: 0.6224 - val_accuracy: 0.7383 - val_sparse_categorical_crossentropy: 0.5221\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 170s 1s/step - loss: 0.6142 - accuracy: 0.7538 - sparse_categorical_crossentropy: 0.5312 - val_loss: 0.5342 - val_accuracy: 0.7908 - val_sparse_categorical_crossentropy: 0.4639\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 187s 1s/step - loss: 0.5002 - accuracy: 0.7963 - sparse_categorical_crossentropy: 0.4432 - val_loss: 0.4923 - val_accuracy: 0.7876 - val_sparse_categorical_crossentropy: 0.4428\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 191s 1s/step - loss: 0.4546 - accuracy: 0.8099 - sparse_categorical_crossentropy: 0.4080 - val_loss: 0.4745 - val_accuracy: 0.7981 - val_sparse_categorical_crossentropy: 0.4307\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 182s 1s/step - loss: 0.4294 - accuracy: 0.8224 - sparse_categorical_crossentropy: 0.3861 - val_loss: 0.4804 - val_accuracy: 0.8037 - val_sparse_categorical_crossentropy: 0.4375\n",
      "Epoch 6/15\n",
      "155/155 [==============================] - 185s 1s/step - loss: 0.4217 - accuracy: 0.8278 - sparse_categorical_crossentropy: 0.3787 - val_loss: 0.4562 - val_accuracy: 0.8199 - val_sparse_categorical_crossentropy: 0.4131\n",
      "Epoch 7/15\n",
      "155/155 [==============================] - 184s 1s/step - loss: 0.4003 - accuracy: 0.8378 - sparse_categorical_crossentropy: 0.3571 - val_loss: 0.4533 - val_accuracy: 0.8134 - val_sparse_categorical_crossentropy: 0.4092\n",
      "Epoch 8/15\n",
      "155/155 [==============================] - 156s 1s/step - loss: 0.3865 - accuracy: 0.8452 - sparse_categorical_crossentropy: 0.3420 - val_loss: 0.4478 - val_accuracy: 0.8279 - val_sparse_categorical_crossentropy: 0.4033\n",
      "Epoch 9/15\n",
      "155/155 [==============================] - 156s 1s/step - loss: 0.3742 - accuracy: 0.8528 - sparse_categorical_crossentropy: 0.3281 - val_loss: 0.4385 - val_accuracy: 0.8288 - val_sparse_categorical_crossentropy: 0.3919\n",
      "Epoch 10/15\n",
      "155/155 [==============================] - 157s 1s/step - loss: 0.3631 - accuracy: 0.8591 - sparse_categorical_crossentropy: 0.3147 - val_loss: 0.4459 - val_accuracy: 0.8247 - val_sparse_categorical_crossentropy: 0.3966\n",
      "Epoch 11/15\n",
      "155/155 [==============================] - 159s 1s/step - loss: 0.3485 - accuracy: 0.8676 - sparse_categorical_crossentropy: 0.2975 - val_loss: 0.4522 - val_accuracy: 0.8279 - val_sparse_categorical_crossentropy: 0.3996\n",
      "39/39 [==============================] - 5s 133ms/step - loss: 0.4385 - accuracy: 0.8288 - sparse_categorical_crossentropy: 0.3919\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-15\\assets\n",
      "--- Starting Trial: run-16\n",
      "{'kernel size': 5, 'kernel num': 128, 'lambda reg': 1e-05}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 392s 3s/step - loss: 0.6604 - accuracy: 0.7101 - sparse_categorical_crossentropy: 0.6538 - val_loss: 0.4985 - val_accuracy: 0.7763 - val_sparse_categorical_crossentropy: 0.4928\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 394s 3s/step - loss: 0.4681 - accuracy: 0.7911 - sparse_categorical_crossentropy: 0.4628 - val_loss: 0.4470 - val_accuracy: 0.8053 - val_sparse_categorical_crossentropy: 0.4421\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 408s 3s/step - loss: 0.4263 - accuracy: 0.8065 - sparse_categorical_crossentropy: 0.4215 - val_loss: 0.4152 - val_accuracy: 0.8078 - val_sparse_categorical_crossentropy: 0.4104\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 414s 3s/step - loss: 0.3972 - accuracy: 0.8265 - sparse_categorical_crossentropy: 0.3922 - val_loss: 0.4265 - val_accuracy: 0.8142 - val_sparse_categorical_crossentropy: 0.4214\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 415s 3s/step - loss: 0.3640 - accuracy: 0.8365 - sparse_categorical_crossentropy: 0.3587 - val_loss: 0.4002 - val_accuracy: 0.8094 - val_sparse_categorical_crossentropy: 0.3946\n",
      "Epoch 6/15\n",
      "155/155 [==============================] - 398s 3s/step - loss: 0.3461 - accuracy: 0.8432 - sparse_categorical_crossentropy: 0.3403 - val_loss: 0.4161 - val_accuracy: 0.8086 - val_sparse_categorical_crossentropy: 0.4101\n",
      "Epoch 7/15\n",
      "155/155 [==============================] - 395s 3s/step - loss: 0.3251 - accuracy: 0.8574 - sparse_categorical_crossentropy: 0.3189 - val_loss: 0.3951 - val_accuracy: 0.8296 - val_sparse_categorical_crossentropy: 0.3887\n",
      "Epoch 8/15\n",
      "155/155 [==============================] - 391s 3s/step - loss: 0.2890 - accuracy: 0.8712 - sparse_categorical_crossentropy: 0.2822 - val_loss: 0.4179 - val_accuracy: 0.8360 - val_sparse_categorical_crossentropy: 0.4109\n",
      "Epoch 9/15\n",
      "155/155 [==============================] - 408s 3s/step - loss: 0.2621 - accuracy: 0.8865 - sparse_categorical_crossentropy: 0.2548 - val_loss: 0.4120 - val_accuracy: 0.8360 - val_sparse_categorical_crossentropy: 0.4044\n",
      "39/39 [==============================] - 13s 328ms/step - loss: 0.3951 - accuracy: 0.8296 - sparse_categorical_crossentropy: 0.3887\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-16\\assets\n",
      "--- Starting Trial: run-17\n",
      "{'kernel size': 5, 'kernel num': 128, 'lambda reg': 0.0001}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 395s 3s/step - loss: 0.8127 - accuracy: 0.6196 - sparse_categorical_crossentropy: 0.7688 - val_loss: 0.7071 - val_accuracy: 0.5929 - val_sparse_categorical_crossentropy: 0.6741\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 412s 3s/step - loss: 0.6146 - accuracy: 0.7076 - sparse_categorical_crossentropy: 0.5857 - val_loss: 0.5895 - val_accuracy: 0.7367 - val_sparse_categorical_crossentropy: 0.5634\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 410s 3s/step - loss: 0.6272 - accuracy: 0.6825 - sparse_categorical_crossentropy: 0.6028 - val_loss: 0.7010 - val_accuracy: 0.5872 - val_sparse_categorical_crossentropy: 0.6775\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 392s 3s/step - loss: 0.6990 - accuracy: 0.5848 - sparse_categorical_crossentropy: 0.6776 - val_loss: 0.6914 - val_accuracy: 0.5977 - val_sparse_categorical_crossentropy: 0.6719\n",
      "39/39 [==============================] - 15s 378ms/step - loss: 0.5895 - accuracy: 0.7367 - sparse_categorical_crossentropy: 0.5634\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-17\\assets\n",
      "--- Starting Trial: run-18\n",
      "{'kernel size': 5, 'kernel num': 128, 'lambda reg': 0.001}\n",
      "Epoch 1/15\n",
      "155/155 [==============================] - 405s 3s/step - loss: 0.9741 - accuracy: 0.7077 - sparse_categorical_crossentropy: 0.7068 - val_loss: 0.6356 - val_accuracy: 0.7754 - val_sparse_categorical_crossentropy: 0.4771\n",
      "Epoch 2/15\n",
      "155/155 [==============================] - 431s 3s/step - loss: 0.5910 - accuracy: 0.7882 - sparse_categorical_crossentropy: 0.4743 - val_loss: 0.5582 - val_accuracy: 0.7932 - val_sparse_categorical_crossentropy: 0.4684\n",
      "Epoch 3/15\n",
      "155/155 [==============================] - 412s 3s/step - loss: 0.5284 - accuracy: 0.7963 - sparse_categorical_crossentropy: 0.4528 - val_loss: 0.4935 - val_accuracy: 0.7989 - val_sparse_categorical_crossentropy: 0.4280\n",
      "Epoch 4/15\n",
      "155/155 [==============================] - 389s 3s/step - loss: 0.4711 - accuracy: 0.8150 - sparse_categorical_crossentropy: 0.4121 - val_loss: 0.4704 - val_accuracy: 0.8086 - val_sparse_categorical_crossentropy: 0.4161\n",
      "Epoch 5/15\n",
      "155/155 [==============================] - 402s 3s/step - loss: 0.4666 - accuracy: 0.8117 - sparse_categorical_crossentropy: 0.4135 - val_loss: 0.4531 - val_accuracy: 0.8199 - val_sparse_categorical_crossentropy: 0.4008\n",
      "Epoch 6/15\n",
      "155/155 [==============================] - 383s 2s/step - loss: 0.4320 - accuracy: 0.8298 - sparse_categorical_crossentropy: 0.3806 - val_loss: 0.4685 - val_accuracy: 0.8150 - val_sparse_categorical_crossentropy: 0.4183\n",
      "Epoch 7/15\n",
      "155/155 [==============================] - 378s 2s/step - loss: 0.4061 - accuracy: 0.8407 - sparse_categorical_crossentropy: 0.3565 - val_loss: 0.4607 - val_accuracy: 0.8215 - val_sparse_categorical_crossentropy: 0.4122\n",
      "39/39 [==============================] - 13s 327ms/step - loss: 0.4531 - accuracy: 0.8199 - sparse_categorical_crossentropy: 0.4008\n",
      "INFO:tensorflow:Assets written to: saved_models\\Model 1 (Shoes_Gender_L2)\\Run-18\\assets\n"
     ]
    }
   ],
   "source": [
    "session_num = 1\n",
    "for kernel_size in HP_KERNEL_SIZE.domain.values:\n",
    "    for kernel_num in HP_KERNEL_NUM.domain.values:\n",
    "        for lambda_reg in HP_LAMBDA_REG.domain.values:\n",
    "            hparams = {\n",
    "                HP_KERNEL_SIZE : kernel_size,\n",
    "                HP_KERNEL_NUM : kernel_num,\n",
    "                HP_LAMBDA_REG : lambda_reg\n",
    "            }\n",
    "            run_name = f'run-{session_num}'\n",
    "            print('--- Starting Trial:',run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            run(\"Logs/Model 1 (Shoes_Gender_L2)/hparam_tuning/\" + run_name, hparams, session_num)\n",
    "\n",
    "            session_num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TF2)",
   "language": "python",
   "name": "python3-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
